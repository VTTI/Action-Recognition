configFile: "configs/detection/ava/slowonly_kinetics_pretrained_r101_8x8x1_20e_ava_rgb.py" # Path to config file
checkpoint: "https://download.openmmlab.com/mmaction/detection/ava/slowonly_kinetics_pretrained_r101_8x8x1_20e_ava_rgb/slowonly_kinetics_pretrained_r101_8x8x1_20e_ava_rgb_20201217-1c9b4117.pth" # Link to checkpoint file
detConfig: "demo/faster_rcnn_r50_fpn_2x_coco.py" # Path to config file for detection
detCheckpoint: "http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth" # Link to checkpoint file for detection
labelMap: "tools/data/ava/label_map.txt" # Path to label file
inputStep: 1 # Input step for sampling frames 
actionThreshold: 0.5 # The threshold of human action score			     
detThreshold: 0.9 # Human detection score threshold
stride: 0 # The prediction stride equals to stride * sample_length (sample_length indicates the size of temporal window from which you sample frames, which equals to clip_len x frame_interval). If set as 0, the prediction stride is 1
predictStepsize: 8 # Give out a prediction per n frames
outputStepsize: 4 # Show one frame per n frames in the demo. We should have (predictStepsize) % (outputStepsize) == 0
outputFPS: 6 # The fps of video output
writeVideo: True # Flag for whether to write output video
